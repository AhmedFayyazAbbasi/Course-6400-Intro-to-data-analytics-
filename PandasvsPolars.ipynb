{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set working directory\n",
        "%cd /content/drive/MyDrive/Research/Datasets/"
      ],
      "metadata": {
        "id": "bp-rLQqGaJ1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Performance Comparison: Pandas vs Polars (Eager & Lazy)\n",
        "========================================================\n",
        "Using NYC Motor Vehicle Collisions Dataset\n",
        "\n",
        "This script benchmarks various data operations across three approaches:\n",
        "1. Pandas (eager loading)\n",
        "2. Polars (eager loading)\n",
        "3. Polars (lazy loading)\n",
        "\n",
        "Dataset: NYC Motor Vehicle Collisions\n",
        "Download from: https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95\n",
        "Save as: 'Motor_Vehicle_Collisions_Crashes.csv'\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "from functools import wraps\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "CSV_FILE = \"nyccollision.csv\"\n",
        "\n",
        "# Column names as per the dataset\n",
        "COLS = {\n",
        "    \"date\": \"CRASH DATE\",\n",
        "    \"time\": \"CRASH TIME\",\n",
        "    \"borough\": \"BOROUGH\",\n",
        "    \"zip\": \"ZIP CODE\",\n",
        "    \"lat\": \"LATITUDE\",\n",
        "    \"lon\": \"LONGITUDE\",\n",
        "    \"location\": \"LOCATION\",\n",
        "    \"on_street\": \"ON STREET NAME\",\n",
        "    \"cross_street\": \"CROSS STREET NAME\",\n",
        "    \"off_street\": \"OFF STREET NAME\",\n",
        "    \"persons_injured\": \"NUMBER OF PERSONS INJURED\",\n",
        "    \"persons_killed\": \"NUMBER OF PERSONS KILLED\",\n",
        "    \"pedestrians_injured\": \"NUMBER OF PEDESTRIANS INJURED\",\n",
        "    \"pedestrians_killed\": \"NUMBER OF PEDESTRIANS KILLED\",\n",
        "    \"cyclists_injured\": \"NUMBER OF CYCLIST INJURED\",\n",
        "    \"cyclists_killed\": \"NUMBER OF CYCLIST KILLED\",\n",
        "    \"motorists_injured\": \"NUMBER OF MOTORIST INJURED\",\n",
        "    \"motorists_killed\": \"NUMBER OF MOTORIST KILLED\",\n",
        "    \"factor_1\": \"CONTRIBUTING FACTOR VEHICLE 1\",\n",
        "    \"factor_2\": \"CONTRIBUTING FACTOR VEHICLE 2\",\n",
        "    \"factor_3\": \"CONTRIBUTING FACTOR VEHICLE 3\",\n",
        "    \"factor_4\": \"CONTRIBUTING FACTOR VEHICLE 4\",\n",
        "    \"factor_5\": \"CONTRIBUTING FACTOR VEHICLE 5\",\n",
        "    \"collision_id\": \"COLLISION_ID\",\n",
        "    \"vehicle_1\": \"VEHICLE TYPE CODE 1\",\n",
        "    \"vehicle_2\": \"VEHICLE TYPE CODE 2\",\n",
        "    \"vehicle_3\": \"VEHICLE TYPE CODE 3\",\n",
        "    \"vehicle_4\": \"VEHICLE TYPE CODE 4\",\n",
        "    \"vehicle_5\": \"VEHICLE TYPE CODE 5\",\n",
        "}\n",
        "\n",
        "def timer(func):\n",
        "    \"\"\"Decorator to measure execution time\"\"\"\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.perf_counter()\n",
        "        result = func(*args, **kwargs)\n",
        "        end = time.perf_counter()\n",
        "        return result, end - start\n",
        "    return wrapper\n",
        "\n",
        "def print_results(operation, times):\n",
        "    \"\"\"Pretty print benchmark results\"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Operation: {operation}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"  Pandas (Eager):       {times['pandas']:>10.4f} seconds\")\n",
        "    print(f\"  Polars (Eager):       {times['polars_eager']:>10.4f} seconds\")\n",
        "    print(f\"  Polars (Lazy):        {times['polars_lazy']:>10.4f} seconds\")\n",
        "    print(f\"  ---\")\n",
        "    speedup_eager = times['pandas'] / times['polars_eager'] if times['polars_eager'] > 0 else float('inf')\n",
        "    speedup_lazy = times['pandas'] / times['polars_lazy'] if times['polars_lazy'] > 0 else float('inf')\n",
        "    print(f\"  Polars Eager Speedup: {speedup_eager:>10.2f}x faster\")\n",
        "    print(f\"  Polars Lazy Speedup:  {speedup_lazy:>10.2f}x faster\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"NYC MOTOR VEHICLE COLLISIONS - PANDAS vs POLARS BENCHMARK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# =============================================================================\n",
        "# 1. BENCHMARK: READING CSV\n",
        "# =============================================================================\n",
        "print(\"\\n[1/12] Benchmarking: Reading CSV File...\")\n",
        "\n",
        "@timer\n",
        "def read_pandas():\n",
        "    return pd.read_csv(CSV_FILE)\n",
        "\n",
        "@timer\n",
        "def read_polars_eager():\n",
        "    return pl.read_csv(CSV_FILE)\n",
        "\n",
        "@timer\n",
        "def read_polars_lazy():\n",
        "    return pl.scan_csv(CSV_FILE).collect()\n",
        "\n",
        "_, t_pandas = read_pandas()\n",
        "_, t_polars_eager = read_polars_eager()\n",
        "_, t_polars_lazy = read_polars_lazy()\n",
        "\n",
        "print_results(\"Reading CSV File\", {\n",
        "    \"pandas\": t_pandas,\n",
        "    \"polars_eager\": t_polars_eager,\n",
        "    \"polars_lazy\": t_polars_lazy\n",
        "})\n",
        "\n",
        "# Load data into memory for subsequent benchmarks\n",
        "print(\"\\nLoading data into memory for benchmarks...\")\n",
        "df_pandas = pd.read_csv(CSV_FILE)\n",
        "df_polars = pl.read_csv(CSV_FILE)\n",
        "lf_polars = pl.scan_csv(CSV_FILE)\n",
        "\n",
        "print(f\"Dataset loaded: {len(df_pandas):,} rows × {len(df_pandas.columns)} columns\")\n",
        "\n"
      ],
      "metadata": {
        "id": "UZmn1x0mZ_f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pipeline Operations**\n",
        "  1.  Read CSV file\n",
        "  2.  Filter invalid/null records\n",
        "  3.  Parse dates & extract year/month/weekday/hour\n",
        "  4.  Fill null values in numeric columns\n",
        "  5.  Calculate: total_injured, total_killed, severity_score\n",
        "  6.  Categorize: severity_category, victim_type, time_of_day\n",
        "  7.  String operations: uppercase, regex matching\n",
        "  8.  Complex 5-level groupby with 12 aggregations\n",
        "  9.  Post-aggregation calculations (rates, percentages)\n",
        "  10. Multi-column sort & top 100 selection"
      ],
      "metadata": {
        "id": "LYcv6LWUd82u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pandas_start = time.perf_counter()\n",
        "\n",
        "# Step 1: Read CSV\n",
        "step1_start = time.perf_counter()\n",
        "df_pd = pd.read_csv(CSV_FILE)\n",
        "step1_time = time.perf_counter() - step1_start\n",
        "print(f\"  Step 1  - Read CSV:                    {step1_time:.4f}s  |  Rows: {len(df_pd):,}\")\n",
        "\n",
        "# Step 2: Clean data - Remove rows with missing critical fields\n",
        "step2_start = time.perf_counter()\n",
        "df_pd = df_pd[\n",
        "    (df_pd[\"BOROUGH\"].notna()) &\n",
        "    (df_pd[\"CONTRIBUTING FACTOR VEHICLE 1\"].notna()) &\n",
        "    (df_pd[\"CONTRIBUTING FACTOR VEHICLE 1\"] != \"Unspecified\") &\n",
        "    (df_pd[\"CRASH DATE\"].notna())\n",
        "].copy()\n",
        "step2_time = time.perf_counter() - step2_start\n",
        "print(f\"  Step 2  - Clean data (filter nulls):   {step2_time:.4f}s  |  Rows: {len(df_pd):,}\")\n",
        "\n",
        "# Step 3: Parse date and extract components\n",
        "step3_start = time.perf_counter()\n",
        "df_pd[\"CRASH DATE\"] = pd.to_datetime(df_pd[\"CRASH DATE\"])\n",
        "df_pd[\"crash_year\"] = df_pd[\"CRASH DATE\"].dt.year\n",
        "df_pd[\"crash_month\"] = df_pd[\"CRASH DATE\"].dt.month\n",
        "df_pd[\"crash_day_of_week\"] = df_pd[\"CRASH DATE\"].dt.dayofweek\n",
        "df_pd[\"crash_hour\"] = pd.to_datetime(df_pd[\"CRASH TIME\"], format=\"%H:%M\", errors=\"coerce\").dt.hour\n",
        "step3_time = time.perf_counter() - step3_start\n",
        "print(f\"  Step 3  - Parse dates & extract:       {step3_time:.4f}s\")\n",
        "\n",
        "# Step 4: Fill null numeric values\n",
        "step4_start = time.perf_counter()\n",
        "injury_cols = [\n",
        "    \"NUMBER OF PERSONS INJURED\", \"NUMBER OF PERSONS KILLED\",\n",
        "    \"NUMBER OF PEDESTRIANS INJURED\", \"NUMBER OF PEDESTRIANS KILLED\",\n",
        "    \"NUMBER OF CYCLIST INJURED\", \"NUMBER OF CYCLIST KILLED\",\n",
        "    \"NUMBER OF MOTORIST INJURED\", \"NUMBER OF MOTORIST KILLED\"\n",
        "]\n",
        "df_pd[injury_cols] = df_pd[injury_cols].fillna(0)\n",
        "step4_time = time.perf_counter() - step4_start\n",
        "print(f\"  Step 4  - Fill null values:            {step4_time:.4f}s\")\n",
        "\n",
        "# Step 5: Create calculated columns\n",
        "step5_start = time.perf_counter()\n",
        "df_pd[\"total_injured\"] = (\n",
        "    df_pd[\"NUMBER OF PEDESTRIANS INJURED\"] +\n",
        "    df_pd[\"NUMBER OF CYCLIST INJURED\"] +\n",
        "    df_pd[\"NUMBER OF MOTORIST INJURED\"]\n",
        ")\n",
        "df_pd[\"total_killed\"] = (\n",
        "    df_pd[\"NUMBER OF PEDESTRIANS KILLED\"] +\n",
        "    df_pd[\"NUMBER OF CYCLIST KILLED\"] +\n",
        "    df_pd[\"NUMBER OF MOTORIST KILLED\"]\n",
        ")\n",
        "df_pd[\"total_casualties\"] = df_pd[\"total_injured\"] + df_pd[\"total_killed\"]\n",
        "df_pd[\"severity_score\"] = df_pd[\"total_killed\"] * 100 + df_pd[\"total_injured\"] * 10\n",
        "df_pd[\"has_injuries\"] = df_pd[\"total_injured\"] > 0\n",
        "df_pd[\"has_fatalities\"] = df_pd[\"total_killed\"] > 0\n",
        "step5_time = time.perf_counter() - step5_start\n",
        "print(f\"  Step 5  - Create calculated columns:   {step5_time:.4f}s\")\n",
        "\n",
        "# Step 6: Apply conditional categorization\n",
        "step6_start = time.perf_counter()\n",
        "conditions_severity = [\n",
        "    df_pd[\"total_killed\"] > 0,\n",
        "    df_pd[\"total_injured\"] >= 5,\n",
        "    df_pd[\"total_injured\"] >= 2,\n",
        "    df_pd[\"total_injured\"] >= 1,\n",
        "]\n",
        "choices_severity = [\"Fatal\", \"Critical\", \"Serious\", \"Minor\"]\n",
        "df_pd[\"severity_category\"] = np.select(conditions_severity, choices_severity, default=\"No Injury\")\n",
        "\n",
        "conditions_victim = [\n",
        "    (df_pd[\"NUMBER OF PEDESTRIANS INJURED\"] + df_pd[\"NUMBER OF PEDESTRIANS KILLED\"]) > 0,\n",
        "    (df_pd[\"NUMBER OF CYCLIST INJURED\"] + df_pd[\"NUMBER OF CYCLIST KILLED\"]) > 0,\n",
        "]\n",
        "choices_victim = [\"Pedestrian\", \"Cyclist\"]\n",
        "df_pd[\"primary_victim_type\"] = np.select(conditions_victim, choices_victim, default=\"Motorist/Other\")\n",
        "\n",
        "conditions_time = [\n",
        "    (df_pd[\"crash_hour\"] >= 6) & (df_pd[\"crash_hour\"] < 12),\n",
        "    (df_pd[\"crash_hour\"] >= 12) & (df_pd[\"crash_hour\"] < 18),\n",
        "    (df_pd[\"crash_hour\"] >= 18) & (df_pd[\"crash_hour\"] < 22),\n",
        "]\n",
        "choices_time = [\"Morning\", \"Afternoon\", \"Evening\"]\n",
        "df_pd[\"time_of_day\"] = np.select(conditions_time, choices_time, default=\"Night\")\n",
        "step6_time = time.perf_counter() - step6_start\n",
        "print(f\"  Step 6  - Conditional categorization:  {step6_time:.4f}s\")\n",
        "\n",
        "# Step 7: String operations on contributing factors\n",
        "step7_start = time.perf_counter()\n",
        "df_pd[\"factor_category\"] = df_pd[\"CONTRIBUTING FACTOR VEHICLE 1\"].str.upper()\n",
        "df_pd[\"is_driver_error\"] = df_pd[\"CONTRIBUTING FACTOR VEHICLE 1\"].str.contains(\n",
        "    \"Driver|Distraction|Fatigue|Alcohol|Drugs\", case=False, regex=True, na=False\n",
        ")\n",
        "df_pd[\"is_vehicle_issue\"] = df_pd[\"CONTRIBUTING FACTOR VEHICLE 1\"].str.contains(\n",
        "    \"Brake|Steering|Tire|Light|Vehicle\", case=False, regex=True, na=False\n",
        ")\n",
        "step7_time = time.perf_counter() - step7_start\n",
        "print(f\"  Step 7  - String operations:           {step7_time:.4f}s\")\n",
        "\n",
        "# Step 8: Complex multi-level grouping with aggregations\n",
        "step8_start = time.perf_counter()\n",
        "result_pd = df_pd.groupby(\n",
        "    [\"BOROUGH\", \"crash_year\", \"severity_category\", \"primary_victim_type\", \"time_of_day\"]\n",
        ").agg(\n",
        "    total_crashes=(\"COLLISION_ID\", \"count\"),\n",
        "    total_injured=(\"total_injured\", \"sum\"),\n",
        "    total_killed=(\"total_killed\", \"sum\"),\n",
        "    total_casualties=(\"total_casualties\", \"sum\"),\n",
        "    avg_severity_score=(\"severity_score\", \"mean\"),\n",
        "    max_severity_score=(\"severity_score\", \"max\"),\n",
        "    pedestrians_injured=(\"NUMBER OF PEDESTRIANS INJURED\", \"sum\"),\n",
        "    cyclists_injured=(\"NUMBER OF CYCLIST INJURED\", \"sum\"),\n",
        "    motorists_injured=(\"NUMBER OF MOTORIST INJURED\", \"sum\"),\n",
        "    driver_error_count=(\"is_driver_error\", \"sum\"),\n",
        "    vehicle_issue_count=(\"is_vehicle_issue\", \"sum\"),\n",
        "    unique_factors=(\"CONTRIBUTING FACTOR VEHICLE 1\", \"nunique\"),\n",
        ").reset_index()\n",
        "step8_time = time.perf_counter() - step8_start\n",
        "print(f\"  Step 8  - Complex grouping & agg:      {step8_time:.4f}s  |  Groups: {len(result_pd):,}\")\n",
        "\n",
        "# Step 9: Post-aggregation calculations\n",
        "step9_start = time.perf_counter()\n",
        "result_pd[\"casualty_rate\"] = result_pd[\"total_casualties\"] / result_pd[\"total_crashes\"]\n",
        "result_pd[\"fatality_rate\"] = result_pd[\"total_killed\"] / result_pd[\"total_crashes\"]\n",
        "result_pd[\"driver_error_pct\"] = result_pd[\"driver_error_count\"] / result_pd[\"total_crashes\"] * 100\n",
        "step9_time = time.perf_counter() - step9_start\n",
        "print(f\"  Step 9  - Post-aggregation calcs:      {step9_time:.4f}s\")\n",
        "\n",
        "# Step 10: Sort by severity and get top results\n",
        "step10_start = time.perf_counter()\n",
        "result_pd = result_pd.sort_values(\n",
        "    [\"total_casualties\", \"total_crashes\", \"avg_severity_score\"],\n",
        "    ascending=[False, False, False]\n",
        ")\n",
        "top_100_pd = result_pd.head(100)\n",
        "step10_time = time.perf_counter() - step10_start\n",
        "print(f\"  Step 10 - Sort & get top 100:          {step10_time:.4f}s\")\n",
        "\n",
        "pandas_total = time.perf_counter() - pandas_start\n",
        "print(f\"\\n  >>> PANDAS TOTAL TIME: {pandas_total:.4f} seconds <<<\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aX94_lEfdJLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# POLARS (EAGER LOADING)\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"POLARS (EAGER LOADING)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "polars_eager_start = time.perf_counter()\n",
        "\n",
        "# Step 1: Read CSV\n",
        "step1_start = time.perf_counter()\n",
        "df_pl = pl.read_csv(CSV_FILE)\n",
        "step1_time = time.perf_counter() - step1_start\n",
        "print(f\"  Step 1  - Read CSV:                    {step1_time:.4f}s  |  Rows: {len(df_pl):,}\")\n",
        "\n",
        "# Step 2: Clean data - Remove rows with missing critical fields\n",
        "step2_start = time.perf_counter()\n",
        "df_pl = df_pl.filter(\n",
        "    (pl.col(\"BOROUGH\").is_not_null()) &\n",
        "    (pl.col(\"CONTRIBUTING FACTOR VEHICLE 1\").is_not_null()) &\n",
        "    (pl.col(\"CONTRIBUTING FACTOR VEHICLE 1\") != \"Unspecified\") &\n",
        "    (pl.col(\"CRASH DATE\").is_not_null())\n",
        ")\n",
        "step2_time = time.perf_counter() - step2_start\n",
        "print(f\"  Step 2  - Clean data (filter nulls):   {step2_time:.4f}s  |  Rows: {len(df_pl):,}\")\n",
        "\n",
        "# Step 3: Parse date and extract components\n",
        "step3_start = time.perf_counter()\n",
        "df_pl = df_pl.with_columns([\n",
        "    pl.col(\"CRASH DATE\").str.to_datetime(\"%m/%d/%Y\").alias(\"CRASH DATE\"),\n",
        "])\n",
        "df_pl = df_pl.with_columns([\n",
        "    pl.col(\"CRASH DATE\").dt.year().alias(\"crash_year\"),\n",
        "    pl.col(\"CRASH DATE\").dt.month().alias(\"crash_month\"),\n",
        "    pl.col(\"CRASH DATE\").dt.weekday().alias(\"crash_day_of_week\"),\n",
        "    pl.col(\"CRASH TIME\").str.slice(0, 2).cast(pl.Int32, strict=False).alias(\"crash_hour\"),\n",
        "])\n",
        "step3_time = time.perf_counter() - step3_start\n",
        "print(f\"  Step 3  - Parse dates & extract:       {step3_time:.4f}s\")\n",
        "\n",
        "# Step 4: Fill null numeric values\n",
        "step4_start = time.perf_counter()\n",
        "injury_cols = [\n",
        "    \"NUMBER OF PERSONS INJURED\", \"NUMBER OF PERSONS KILLED\",\n",
        "    \"NUMBER OF PEDESTRIANS INJURED\", \"NUMBER OF PEDESTRIANS KILLED\",\n",
        "    \"NUMBER OF CYCLIST INJURED\", \"NUMBER OF CYCLIST KILLED\",\n",
        "    \"NUMBER OF MOTORIST INJURED\", \"NUMBER OF MOTORIST KILLED\"\n",
        "]\n",
        "df_pl = df_pl.with_columns([pl.col(c).fill_null(0) for c in injury_cols])\n",
        "step4_time = time.perf_counter() - step4_start\n",
        "print(f\"  Step 4  - Fill null values:            {step4_time:.4f}s\")\n",
        "\n",
        "# Step 5: Create calculated columns\n",
        "step5_start = time.perf_counter()\n",
        "df_pl = df_pl.with_columns([\n",
        "    (pl.col(\"NUMBER OF PEDESTRIANS INJURED\") +\n",
        "     pl.col(\"NUMBER OF CYCLIST INJURED\") +\n",
        "     pl.col(\"NUMBER OF MOTORIST INJURED\")).alias(\"total_injured\"),\n",
        "    (pl.col(\"NUMBER OF PEDESTRIANS KILLED\") +\n",
        "     pl.col(\"NUMBER OF CYCLIST KILLED\") +\n",
        "     pl.col(\"NUMBER OF MOTORIST KILLED\")).alias(\"total_killed\"),\n",
        "])\n",
        "df_pl = df_pl.with_columns([\n",
        "    (pl.col(\"total_injured\") + pl.col(\"total_killed\")).alias(\"total_casualties\"),\n",
        "    (pl.col(\"total_killed\") * 100 + pl.col(\"total_injured\") * 10).alias(\"severity_score\"),\n",
        "    (pl.col(\"total_injured\") > 0).alias(\"has_injuries\"),\n",
        "    (pl.col(\"total_killed\") > 0).alias(\"has_fatalities\"),\n",
        "])\n",
        "step5_time = time.perf_counter() - step5_start\n",
        "print(f\"  Step 5  - Create calculated columns:   {step5_time:.4f}s\")\n",
        "\n",
        "# Step 6: Apply conditional categorization\n",
        "step6_start = time.perf_counter()\n",
        "df_pl = df_pl.with_columns([\n",
        "    pl.when(pl.col(\"total_killed\") > 0).then(pl.lit(\"Fatal\"))\n",
        "      .when(pl.col(\"total_injured\") >= 5).then(pl.lit(\"Critical\"))\n",
        "      .when(pl.col(\"total_injured\") >= 2).then(pl.lit(\"Serious\"))\n",
        "      .when(pl.col(\"total_injured\") >= 1).then(pl.lit(\"Minor\"))\n",
        "      .otherwise(pl.lit(\"No Injury\"))\n",
        "      .alias(\"severity_category\"),\n",
        "    pl.when((pl.col(\"NUMBER OF PEDESTRIANS INJURED\") + pl.col(\"NUMBER OF PEDESTRIANS KILLED\")) > 0)\n",
        "      .then(pl.lit(\"Pedestrian\"))\n",
        "      .when((pl.col(\"NUMBER OF CYCLIST INJURED\") + pl.col(\"NUMBER OF CYCLIST KILLED\")) > 0)\n",
        "      .then(pl.lit(\"Cyclist\"))\n",
        "      .otherwise(pl.lit(\"Motorist/Other\"))\n",
        "      .alias(\"primary_victim_type\"),\n",
        "    pl.when((pl.col(\"crash_hour\") >= 6) & (pl.col(\"crash_hour\") < 12)).then(pl.lit(\"Morning\"))\n",
        "      .when((pl.col(\"crash_hour\") >= 12) & (pl.col(\"crash_hour\") < 18)).then(pl.lit(\"Afternoon\"))\n",
        "      .when((pl.col(\"crash_hour\") >= 18) & (pl.col(\"crash_hour\") < 22)).then(pl.lit(\"Evening\"))\n",
        "      .otherwise(pl.lit(\"Night\"))\n",
        "      .alias(\"time_of_day\"),\n",
        "])\n",
        "step6_time = time.perf_counter() - step6_start\n",
        "print(f\"  Step 6  - Conditional categorization:  {step6_time:.4f}s\")\n",
        "\n",
        "# Step 7: String operations on contributing factors\n",
        "step7_start = time.perf_counter()\n",
        "df_pl = df_pl.with_columns([\n",
        "    pl.col(\"CONTRIBUTING FACTOR VEHICLE 1\").str.to_uppercase().alias(\"factor_category\"),\n",
        "    pl.col(\"CONTRIBUTING FACTOR VEHICLE 1\")\n",
        "      .str.contains(\"(?i)Driver|Distraction|Fatigue|Alcohol|Drugs\")\n",
        "      .fill_null(False).alias(\"is_driver_error\"),\n",
        "    pl.col(\"CONTRIBUTING FACTOR VEHICLE 1\")\n",
        "      .str.contains(\"(?i)Brake|Steering|Tire|Light|Vehicle\")\n",
        "      .fill_null(False).alias(\"is_vehicle_issue\"),\n",
        "])\n",
        "step7_time = time.perf_counter() - step7_start\n",
        "print(f\"  Step 7  - String operations:           {step7_time:.4f}s\")\n",
        "\n",
        "# Step 8: Complex multi-level grouping with aggregations\n",
        "step8_start = time.perf_counter()\n",
        "result_pl = df_pl.group_by(\n",
        "    [\"BOROUGH\", \"crash_year\", \"severity_category\", \"primary_victim_type\", \"time_of_day\"]\n",
        ").agg([\n",
        "    pl.col(\"COLLISION_ID\").count().alias(\"total_crashes\"),\n",
        "    pl.col(\"total_injured\").sum().alias(\"total_injured\"),\n",
        "    pl.col(\"total_killed\").sum().alias(\"total_killed\"),\n",
        "    pl.col(\"total_casualties\").sum().alias(\"total_casualties\"),\n",
        "    pl.col(\"severity_score\").mean().alias(\"avg_severity_score\"),\n",
        "    pl.col(\"severity_score\").max().alias(\"max_severity_score\"),\n",
        "    pl.col(\"NUMBER OF PEDESTRIANS INJURED\").sum().alias(\"pedestrians_injured\"),\n",
        "    pl.col(\"NUMBER OF CYCLIST INJURED\").sum().alias(\"cyclists_injured\"),\n",
        "    pl.col(\"NUMBER OF MOTORIST INJURED\").sum().alias(\"motorists_injured\"),\n",
        "    pl.col(\"is_driver_error\").sum().alias(\"driver_error_count\"),\n",
        "    pl.col(\"is_vehicle_issue\").sum().alias(\"vehicle_issue_count\"),\n",
        "    pl.col(\"CONTRIBUTING FACTOR VEHICLE 1\").n_unique().alias(\"unique_factors\"),\n",
        "])\n",
        "step8_time = time.perf_counter() - step8_start\n",
        "print(f\"  Step 8  - Complex grouping & agg:      {step8_time:.4f}s  |  Groups: {len(result_pl):,}\")\n",
        "\n",
        "# Step 9: Post-aggregation calculations\n",
        "step9_start = time.perf_counter()\n",
        "result_pl = result_pl.with_columns([\n",
        "    (pl.col(\"total_casualties\") / pl.col(\"total_crashes\")).alias(\"casualty_rate\"),\n",
        "    (pl.col(\"total_killed\") / pl.col(\"total_crashes\")).alias(\"fatality_rate\"),\n",
        "    (pl.col(\"driver_error_count\") / pl.col(\"total_crashes\") * 100).alias(\"driver_error_pct\"),\n",
        "])\n",
        "step9_time = time.perf_counter() - step9_start\n",
        "print(f\"  Step 9  - Post-aggregation calcs:      {step9_time:.4f}s\")\n",
        "\n",
        "# Step 10: Sort by severity and get top results\n",
        "step10_start = time.perf_counter()\n",
        "result_pl = result_pl.sort(\n",
        "    [\"total_casualties\", \"total_crashes\", \"avg_severity_score\"],\n",
        "    descending=[True, True, True]\n",
        ")\n",
        "top_100_pl = result_pl.head(100)\n",
        "step10_time = time.perf_counter() - step10_start\n",
        "print(f\"  Step 10 - Sort & get top 100:          {step10_time:.4f}s\")\n",
        "\n",
        "polars_eager_total = time.perf_counter() - polars_eager_start\n",
        "print(f\"\\n  >>> POLARS EAGER TOTAL TIME: {polars_eager_total:.4f} seconds <<<\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_E40a79OdWn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# POLARS (LAZY LOADING)\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"POLARS (LAZY LOADING)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "polars_lazy_start = time.perf_counter()\n",
        "\n",
        "# All steps combined in a single lazy query\n",
        "print(\"  Building query plan...\")\n",
        "\n",
        "query_start = time.perf_counter()\n",
        "\n",
        "# Build the complete lazy query\n",
        "lazy_result = (\n",
        "    # Step 1: Scan CSV (lazy)\n",
        "    pl.scan_csv(CSV_FILE)\n",
        "\n",
        "    # Step 2: Clean data\n",
        "    .filter(\n",
        "        (pl.col(\"BOROUGH\").is_not_null()) &\n",
        "        (pl.col(\"CONTRIBUTING FACTOR VEHICLE 1\").is_not_null()) &\n",
        "        (pl.col(\"CONTRIBUTING FACTOR VEHICLE 1\") != \"Unspecified\") &\n",
        "        (pl.col(\"CRASH DATE\").is_not_null())\n",
        "    )\n",
        "\n",
        "    # Step 3: Parse date and extract components\n",
        "    .with_columns([\n",
        "        pl.col(\"CRASH DATE\").str.to_datetime(\"%m/%d/%Y\").alias(\"CRASH DATE\"),\n",
        "    ])\n",
        "    .with_columns([\n",
        "        pl.col(\"CRASH DATE\").dt.year().alias(\"crash_year\"),\n",
        "        pl.col(\"CRASH DATE\").dt.month().alias(\"crash_month\"),\n",
        "        pl.col(\"CRASH DATE\").dt.weekday().alias(\"crash_day_of_week\"),\n",
        "        pl.col(\"CRASH TIME\").str.slice(0, 2).cast(pl.Int32, strict=False).alias(\"crash_hour\"),\n",
        "    ])\n",
        "\n",
        "    # Step 4: Fill null numeric values\n",
        "    .with_columns([\n",
        "        pl.col(\"NUMBER OF PERSONS INJURED\").fill_null(0),\n",
        "        pl.col(\"NUMBER OF PERSONS KILLED\").fill_null(0),\n",
        "        pl.col(\"NUMBER OF PEDESTRIANS INJURED\").fill_null(0),\n",
        "        pl.col(\"NUMBER OF PEDESTRIANS KILLED\").fill_null(0),\n",
        "        pl.col(\"NUMBER OF CYCLIST INJURED\").fill_null(0),\n",
        "        pl.col(\"NUMBER OF CYCLIST KILLED\").fill_null(0),\n",
        "        pl.col(\"NUMBER OF MOTORIST INJURED\").fill_null(0),\n",
        "        pl.col(\"NUMBER OF MOTORIST KILLED\").fill_null(0),\n",
        "    ])\n",
        "\n",
        "    # Step 5: Create calculated columns\n",
        "    .with_columns([\n",
        "        (pl.col(\"NUMBER OF PEDESTRIANS INJURED\") +\n",
        "         pl.col(\"NUMBER OF CYCLIST INJURED\") +\n",
        "         pl.col(\"NUMBER OF MOTORIST INJURED\")).alias(\"total_injured\"),\n",
        "        (pl.col(\"NUMBER OF PEDESTRIANS KILLED\") +\n",
        "         pl.col(\"NUMBER OF CYCLIST KILLED\") +\n",
        "         pl.col(\"NUMBER OF MOTORIST KILLED\")).alias(\"total_killed\"),\n",
        "    ])\n",
        "    .with_columns([\n",
        "        (pl.col(\"total_injured\") + pl.col(\"total_killed\")).alias(\"total_casualties\"),\n",
        "        (pl.col(\"total_killed\") * 100 + pl.col(\"total_injured\") * 10).alias(\"severity_score\"),\n",
        "        (pl.col(\"total_injured\") > 0).alias(\"has_injuries\"),\n",
        "        (pl.col(\"total_killed\") > 0).alias(\"has_fatalities\"),\n",
        "    ])\n",
        "\n",
        "    # Step 6: Apply conditional categorization\n",
        "    .with_columns([\n",
        "        pl.when(pl.col(\"total_killed\") > 0).then(pl.lit(\"Fatal\"))\n",
        "          .when(pl.col(\"total_injured\") >= 5).then(pl.lit(\"Critical\"))\n",
        "          .when(pl.col(\"total_injured\") >= 2).then(pl.lit(\"Serious\"))\n",
        "          .when(pl.col(\"total_injured\") >= 1).then(pl.lit(\"Minor\"))\n",
        "          .otherwise(pl.lit(\"No Injury\"))\n",
        "          .alias(\"severity_category\"),\n",
        "        pl.when((pl.col(\"NUMBER OF PEDESTRIANS INJURED\") + pl.col(\"NUMBER OF PEDESTRIANS KILLED\")) > 0)\n",
        "          .then(pl.lit(\"Pedestrian\"))\n",
        "          .when((pl.col(\"NUMBER OF CYCLIST INJURED\") + pl.col(\"NUMBER OF CYCLIST KILLED\")) > 0)\n",
        "          .then(pl.lit(\"Cyclist\"))\n",
        "          .otherwise(pl.lit(\"Motorist/Other\"))\n",
        "          .alias(\"primary_victim_type\"),\n",
        "        pl.when((pl.col(\"crash_hour\") >= 6) & (pl.col(\"crash_hour\") < 12)).then(pl.lit(\"Morning\"))\n",
        "          .when((pl.col(\"crash_hour\") >= 12) & (pl.col(\"crash_hour\") < 18)).then(pl.lit(\"Afternoon\"))\n",
        "          .when((pl.col(\"crash_hour\") >= 18) & (pl.col(\"crash_hour\") < 22)).then(pl.lit(\"Evening\"))\n",
        "          .otherwise(pl.lit(\"Night\"))\n",
        "          .alias(\"time_of_day\"),\n",
        "    ])\n",
        "\n",
        "    # Step 7: String operations\n",
        "    .with_columns([\n",
        "        pl.col(\"CONTRIBUTING FACTOR VEHICLE 1\").str.to_uppercase().alias(\"factor_category\"),\n",
        "        pl.col(\"CONTRIBUTING FACTOR VEHICLE 1\")\n",
        "          .str.contains(\"(?i)Driver|Distraction|Fatigue|Alcohol|Drugs\")\n",
        "          .fill_null(False).alias(\"is_driver_error\"),\n",
        "        pl.col(\"CONTRIBUTING FACTOR VEHICLE 1\")\n",
        "          .str.contains(\"(?i)Brake|Steering|Tire|Light|Vehicle\")\n",
        "          .fill_null(False).alias(\"is_vehicle_issue\"),\n",
        "    ])\n",
        "\n",
        "    # Step 8: Complex grouping\n",
        "    .group_by([\"BOROUGH\", \"crash_year\", \"severity_category\", \"primary_victim_type\", \"time_of_day\"])\n",
        "    .agg([\n",
        "        pl.col(\"COLLISION_ID\").count().alias(\"total_crashes\"),\n",
        "        pl.col(\"total_injured\").sum().alias(\"total_injured\"),\n",
        "        pl.col(\"total_killed\").sum().alias(\"total_killed\"),\n",
        "        pl.col(\"total_casualties\").sum().alias(\"total_casualties\"),\n",
        "        pl.col(\"severity_score\").mean().alias(\"avg_severity_score\"),\n",
        "        pl.col(\"severity_score\").max().alias(\"max_severity_score\"),\n",
        "        pl.col(\"NUMBER OF PEDESTRIANS INJURED\").sum().alias(\"pedestrians_injured\"),\n",
        "        pl.col(\"NUMBER OF CYCLIST INJURED\").sum().alias(\"cyclists_injured\"),\n",
        "        pl.col(\"NUMBER OF MOTORIST INJURED\").sum().alias(\"motorists_injured\"),\n",
        "        pl.col(\"is_driver_error\").sum().alias(\"driver_error_count\"),\n",
        "        pl.col(\"is_vehicle_issue\").sum().alias(\"vehicle_issue_count\"),\n",
        "        pl.col(\"CONTRIBUTING FACTOR VEHICLE 1\").n_unique().alias(\"unique_factors\"),\n",
        "    ])\n",
        "\n",
        "    # Step 9: Post-aggregation calculations\n",
        "    .with_columns([\n",
        "        (pl.col(\"total_casualties\") / pl.col(\"total_crashes\")).alias(\"casualty_rate\"),\n",
        "        (pl.col(\"total_killed\") / pl.col(\"total_crashes\")).alias(\"fatality_rate\"),\n",
        "        (pl.col(\"driver_error_count\") / pl.col(\"total_crashes\") * 100).alias(\"driver_error_pct\"),\n",
        "    ])\n",
        "\n",
        "    # Step 10: Sort and limit\n",
        "    .sort([\"total_casualties\", \"total_crashes\", \"avg_severity_score\"], descending=[True, True, True])\n",
        "    .head(100)\n",
        ")\n",
        "\n",
        "query_build_time = time.perf_counter() - query_start\n",
        "print(f\"  Query plan built in:                   {query_build_time:.4f}s\")\n",
        "\n",
        "# Execute the query\n",
        "print(\"  Executing optimized query...\")\n",
        "execute_start = time.perf_counter()\n",
        "top_100_lazy = lazy_result.collect()\n",
        "execute_time = time.perf_counter() - execute_start\n",
        "print(f\"  Query execution:                       {execute_time:.4f}s  |  Results: {len(top_100_lazy):,}\")\n",
        "\n",
        "polars_lazy_total = time.perf_counter() - polars_lazy_start\n",
        "print(f\"\\n  >>> POLARS LAZY TOTAL TIME: {polars_lazy_total:.4f} seconds <<<\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "30WIoCAbdayj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# FINAL COMPARISON SUMMARY\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"PERFORMANCE COMPARISON SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\"\"\n",
        "┌─────────────────────────────────────────────────────────────────────────────┐\n",
        "│                         EXECUTION TIME COMPARISON                           │\n",
        "├─────────────────────────────────────────────────────────────────────────────┤\n",
        "│  Pandas (Eager):        {pandas_total:>10.4f} seconds                               │\n",
        "│  Polars (Eager):        {polars_eager_total:>10.4f} seconds                               │\n",
        "│  Polars (Lazy):         {polars_lazy_total:>10.4f} seconds                               │\n",
        "├─────────────────────────────────────────────────────────────────────────────┤\n",
        "│                            SPEEDUP FACTORS                                  │\n",
        "├─────────────────────────────────────────────────────────────────────────────┤\n",
        "│  Polars Eager vs Pandas:  {pandas_total/polars_eager_total:>6.2f}x faster                               │\n",
        "│  Polars Lazy vs Pandas:   {pandas_total/polars_lazy_total:>6.2f}x faster                               │\n",
        "│  Polars Lazy vs Eager:    {polars_eager_total/polars_lazy_total:>6.2f}x faster                               │\n",
        "└─────────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "Pipeline Operations Performed:\n",
        "  1.  Read CSV file\n",
        "  2.  Filter invalid/null records\n",
        "  3.  Parse dates & extract year/month/weekday/hour\n",
        "  4.  Fill null values in numeric columns\n",
        "  5.  Calculate: total_injured, total_killed, severity_score\n",
        "  6.  Categorize: severity_category, victim_type, time_of_day\n",
        "  7.  String operations: uppercase, regex matching\n",
        "  8.  Complex 5-level groupby with 12 aggregations\n",
        "  9.  Post-aggregation calculations (rates, percentages)\n",
        "  10. Multi-column sort & top 100 selection\n",
        "\n",
        "Key Insights:\n",
        "  • Polars Eager benefits from Rust's performance + parallelization\n",
        "  • Polars Lazy additionally optimizes the query plan:\n",
        "    - Predicate pushdown (filters applied early)\n",
        "    - Projection pushdown (only needed columns read)\n",
        "    - Operation reordering for efficiency\n",
        "    - Reduced memory allocations\n",
        "\"\"\")\n",
        "\n",
        "# Verify results match\n",
        "print(\"Verifying results consistency...\")\n",
        "print(f\"  Pandas result shape:       {top_100_pd.shape}\")\n",
        "print(f\"  Polars Eager result shape: {top_100_pl.shape}\")\n",
        "print(f\"  Polars Lazy result shape:  {top_100_lazy.shape}\")"
      ],
      "metadata": {
        "id": "Dp8pN3XRdsNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dask for parallel computing"
      ],
      "metadata": {
        "id": "0N809SL3D2ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.dataframe as dd\n",
        "df = dd.read_csv('nyccollision.csv',  blocksize=100e6, dtype={'CONTRIBUTING FACTOR VEHICLE 3': 'object',\n",
        "       'CONTRIBUTING FACTOR VEHICLE 4': 'object',\n",
        "       'CONTRIBUTING FACTOR VEHICLE 5': 'object',\n",
        "       'NUMBER OF PERSONS INJURED': 'float64',\n",
        "       'VEHICLE TYPE CODE 3': 'object',\n",
        "       'VEHICLE TYPE CODE 4': 'object',\n",
        "       'VEHICLE TYPE CODE 5': 'object',\n",
        "       'NUMBER OF PERSONS KILLED': 'float64',\n",
        "       'ZIP CODE': 'object'})"
      ],
      "metadata": {
        "id": "tOVv1kkKRyUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.visualize()"
      ],
      "metadata": {
        "id": "yXgTEF1TudO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.get_partition(0).compute()"
      ],
      "metadata": {
        "id": "cBfcUxLiwMAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.dataframe as dd\n",
        "df = dd.read_csv('nyccollision.csv',  blocksize=10e6, dtype={'CONTRIBUTING FACTOR VEHICLE 3': 'object',\n",
        "       'CONTRIBUTING FACTOR VEHICLE 4': 'object',\n",
        "       'CONTRIBUTING FACTOR VEHICLE 5': 'object',\n",
        "       'NUMBER OF PERSONS INJURED': 'float64',\n",
        "       'VEHICLE TYPE CODE 3': 'object',\n",
        "       'VEHICLE TYPE CODE 4': 'object',\n",
        "       'VEHICLE TYPE CODE 5': 'object',\n",
        "       'NUMBER OF PERSONS KILLED': 'float64',\n",
        "       'ZIP CODE': 'object'})\n",
        "df.groupby('BOROUGH')['NUMBER OF PERSONS KILLED'].sum().compute()"
      ],
      "metadata": {
        "id": "ShB5ED8vnqVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Space Optimization"
      ],
      "metadata": {
        "id": "99TWWuktFxPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Parquet file format using polars\n",
        "df_polars = pl.read_csv(\"nyccollision.csv\")\n",
        "df_polars.write_parquet(\"/content/drive/MyDrive/Research/Datasets/collision.parquet\")"
      ],
      "metadata": {
        "id": "EkUPKJr8_uvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Parquet file format using polars\n",
        "df_polars = pl.read_parquet(\"collision.parquet\")\n",
        "df_polars"
      ],
      "metadata": {
        "id": "ZmxXFds8AXq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Parquet file format using pandas\n",
        "dfpandas = pd.read_parquet(\"collision.parquet\")\n",
        "df_pandas"
      ],
      "metadata": {
        "id": "FRMTX_UkBhaj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}